{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of Actor_Critic_ForexEnv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ_16GmdW55L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "63e98481-37aa-407a-ac2f-fd0a1801d4d7"
      },
      "source": [
        "!git clone https://github.com/OmarOsman/Reinforcement-Learning-Trading-"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Reinforcement-Learning-Trading-'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 3), reused 30 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8fFmg7kW_JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ada76fc2-0708-4cfa-b037-b62ef30a238a"
      },
      "source": [
        "cd Reinforcement-Learning-Trading-/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Reinforcement-Learning-Trading-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbjFK1j1WylI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import gym_anytrading\n",
        "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions \n",
        "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
        "from actor_critic_discrete import Agent\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-4TX2asWylT",
        "colab_type": "code",
        "outputId": "c94872e3-b879-4995-aae8-d9d73e69dddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "env = gym.make('forex-v0', \n",
        "               frame_bound=(10, 6225), \n",
        "               window_size=10)\n",
        "                                       \n",
        "agent = Agent(alpha=0.0001, \n",
        "              input_dims=[env.frame_bound[0],2], # timestep , number of signal features \n",
        "              gamma=0.99, \n",
        "              n_actions=2, \n",
        "              layer1_size=128, \n",
        "              layer2_size=32)\n",
        "\n",
        "num_episodes = 100 # need big number to able to learn something \n",
        "ep_reward = 0\n",
        "total_reward = 0\n",
        "done = False \n",
        "running_reward = 0\n",
        "log_interval = 10\n",
        "reward_history = []\n",
        "\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    observation = env.reset()\n",
        "    ep_reward = 0\n",
        "    while not done :\n",
        "        action = agent.choose_action(observation)\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        agent.rewards.append(reward)\n",
        "        ep_reward += reward \n",
        "           \n",
        "    agent.learn(observation, reward, done)\n",
        "    done = False   \n",
        "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "    \n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(i_episode, ep_reward, running_reward))\n",
        "    reward_history.append(running_reward)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0\tLast reward: -339.90\tAverage reward: -17.00\n",
            "Episode 10\tLast reward: 792.60\tAverage reward: 243.04\n",
            "Episode 20\tLast reward: 723.50\tAverage reward: 64.89\n",
            "Episode 30\tLast reward: 1276.30\tAverage reward: 100.82\n",
            "Episode 40\tLast reward: 992.30\tAverage reward: 285.29\n",
            "Episode 50\tLast reward: 468.90\tAverage reward: 236.17\n",
            "Episode 60\tLast reward: -462.40\tAverage reward: 108.23\n",
            "Episode 70\tLast reward: -499.60\tAverage reward: 109.48\n",
            "Episode 80\tLast reward: 626.00\tAverage reward: 129.90\n",
            "Episode 90\tLast reward: -153.60\tAverage reward: -98.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blx3GoJdWylb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0fa8508-c4e1-4a8e-a0aa-4fc3e068a87e"
      },
      "source": [
        "#running one example \n",
        "done = False\n",
        "observation = env.reset()\n",
        "ep_reward = 0\n",
        "while not done :\n",
        "    action = agent.choose_action(observation)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    ep_reward += reward \n",
        "    if done:\n",
        "        print(\"info:\", info)\n",
        "    \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info: {'total_reward': 1023.2000000000965, 'total_profit': 0.6484728396200953, 'position': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXQ4njyidlDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5bde7cc-b668-4057-bd79-6ef6d588402b"
      },
      "source": [
        "#running same envorment without Actor Critic \n",
        "done = False\n",
        "org_env = gym.make('forex-v0', frame_bound=(10, 6225), window_size=10)\n",
        "observation = org_env.reset()\n",
        "while True:\n",
        "    action = org_env.action_space.sample()\n",
        "    observation, reward, done, info = org_env.step(action)\n",
        "    if done:\n",
        "        print(\"info:\", info)\n",
        "        break\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "info: {'total_reward': -2210.7000000000116, 'total_profit': 0.5593466581803774, 'position': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-LtzRkUeeZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}